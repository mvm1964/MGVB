{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrqM1iTnGiDdzIZ/szLA0Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvm1964/MGVB/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_NSkYKFisCP",
        "outputId": "999e8c34-7964-4c4c-8ee9-0a6fd02ff45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5euIpKbAi8L6",
        "outputId": "5ad40f75-0ad8-4a4f-d87e-aec6d65210db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhNbBH5pi3Hv",
        "outputId": "0170cb29-47bb-4c0e-e2f5-c1dffd2229b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "enum AA_table {\n",
        "    A, G, V, L, S\n",
        "};\n",
        "\n",
        "__device__ void substr(char *str, char *substring, int start, int end) {\n",
        "    for (int i = start; i <= end; i++) {\n",
        "        substring[i] = str[i];\n",
        "    }\n",
        "    substring[end + 1] = '\\0';\n",
        "}\n",
        "\n",
        "typedef struct PTM {\n",
        "    char name[16];\n",
        "    char site[32];\n",
        "    float mass;\n",
        "} ptm;\n",
        "\n",
        "typedef struct Peptide {\n",
        "    char sequence[1024];\n",
        "    float mass;\n",
        "    int length;\n",
        "    ptm mod[3];\n",
        "} peptide;\n",
        "\n",
        "typedef struct Sequence {\n",
        "    char sequence[1024];\n",
        "} sequence;\n",
        "\n",
        "__global__ void cuda_hello(){\n",
        "    printf(\"Hello World from GPU!\\n\");\n",
        "}\n",
        "\n",
        "__global__ void compute_score(double *spectrum, char *sequence, double *score_table) {\n",
        "\n",
        "}\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "// test computation\n",
        "__global__ void transform_data(float *tbl1, float *tbl2, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) tbl2[i] = 1.0*tbl1[i];\n",
        "\n",
        "}\n",
        "\n",
        "// test substring\n",
        "__global__ void substr_seq(sequence *data, sequence *result, int start, int end, int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < N) substr(data[i].sequence, result[i].sequence, start, end);\n",
        "}\n",
        "\n",
        "void compute_score_on_host(double *spectrum, peptide pep, double *score_table, int spec_len) {\n",
        "    float AA_masses[] = {100.1, 50.2, 169.4, 180.5, 110.5}; // mock table for developing only\n",
        "    int i = 0;\n",
        "    int j = 0;\n",
        "    int pepLen = 60;\n",
        "    int n_matched = 0;\n",
        "    int n_total = 2*(pepLen - 1);\n",
        "    float b_mass_array[pepLen - 1];\n",
        "    float y_mass_array[pepLen - 1];\n",
        "    float all_mass_array[n_total];\n",
        "    char b_fr_seq[pepLen];\n",
        "    char y_fr_seq[pepLen];\n",
        "    float b_fr_mass = 0;\n",
        "    float y_fr_mass = 0;\n",
        "\n",
        "    // break first peptide bond and process b1 and cognate y fragment\n",
        "    /*int end_b = 0;\n",
        "\n",
        "    substr(pep.sequence, b_fr_seq, 0, end);\n",
        "\n",
        "    for (i = 0; i < 0 + 1; i++) {\n",
        "        b_fr_mass += AA_masses[b_fr_seq[i]];\n",
        "    }\n",
        "\n",
        "    // now y fragment\n",
        "    int end_y = 60 - 1;  // although it is wastefull it is necessary to make threads work in lockstep\n",
        "    char y_fr_seq[60];\n",
        "    substr(pep.sequence, y_fr_seq, end_b + 1, end_y);\n",
        "\n",
        "    for (i = 0; i < end_y + 1; i++) {\n",
        "        y_fr_mass += AA_masses[y_fr_seq[i]];\n",
        "    }*/\n",
        "\n",
        "    // but best to do in a loop\n",
        "\n",
        "    /*for (i = 0; i < pepLen; i++) {\n",
        "        substr(pep.sequence, b_fr_seq, 0, i + 1);\n",
        "        b_fr_mass = 0;\n",
        "        for (j = 0; j < i + 1; j++) b_fr_mass += AA_masses[b_fr_seq[j]];\n",
        "        b_mass_array[i] = b_fr_mass;\n",
        "\n",
        "        substr(pep.sequence, y_fr_seq, i + 1, pepLen - 1);\n",
        "        y_fr_mass = 0;\n",
        "        for (j = i + 1; j < pepLen; j++) y_fr_mass += AA_masses[y_fr_seq[j]];\n",
        "        y_mass_array[pepLen - i - 1] = y_fr_mass;\n",
        "\n",
        "    }*/\n",
        "\n",
        "    // now merge the two ordered arrays\n",
        "    float tmp = 0;\n",
        "    for (i =0; i < pepLen - 1; i++) {\n",
        "        if (b_mass_array[i] > y_mass_array[i]) {\n",
        "            all_mass_array[i] = y_mass_array[i];\n",
        "            tmp = b_mass_array[i++];\n",
        "            all_mass_array[i] = tmp;\n",
        "        }\n",
        "        else {\n",
        "            all_mass_array[i] = b_mass_array[i];\n",
        "            tmp = y_mass_array[i++];\n",
        "            all_mass_array[i] = tmp;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // now call matching function but this tomorrow\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "int main() {\n",
        "    cuda_hello<<<1,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    //gpuErrchk( cudaPeekAtLastError() );\n",
        "    //gpuErrchk( cudaDeviceSynchronize() );\n",
        "    // no needs for error checking,, just toolset problems\n",
        "\n",
        "    int nElem = 1<<14;\n",
        "\n",
        "    printf(\"nElem = %d\\n\", nElem);\n",
        "\n",
        "    size_t nBytes = nElem*sizeof(float);\n",
        "\n",
        "    // score table to use on GPU\n",
        "    float *h_score_table = (float *)malloc(nBytes);\n",
        "    float *h_new_table = (float *)malloc(nBytes);\n",
        "    for (int i = 0; i < nElem; i++) h_score_table[i] = 0.2*i;\n",
        "\n",
        "    float *d_score_table;\n",
        "    float *d_new_table;\n",
        "    cudaMalloc((float **)&d_score_table, nBytes);\n",
        "    cudaMalloc((float **)&d_new_table, nBytes);\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "    // copy to device\n",
        "    cudaMemcpy(d_score_table, h_score_table, nBytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // process data: use max block size of 512\n",
        "    transform_data<<<32,512>>>(d_score_table, d_new_table, nElem);\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "    // copy results to host\n",
        "    cudaMemcpy(h_new_table, d_new_table, nBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    // check if correct\n",
        "    float difference = 0;\n",
        "    for (int i = 0; i < nElem; i++) {\n",
        "        difference += h_new_table[i] - h_score_table[i];\n",
        "    }\n",
        "\n",
        "    printf(\"Difference = %f\\n\", difference);\n",
        "\n",
        "    for (int i = 0; i < nElem; i++) {\n",
        "        //printf(\"%f\\t%f\\n\", h_score_table[i], h_new_table[i]);\n",
        "        if (h_score_table[i] - h_new_table[i]) {\n",
        "            printf(\"%f\\t%f\\n\", h_score_table[i], h_new_table[i]);\n",
        "            printf(\"i = %d\\n\", i);\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    free(h_score_table);\n",
        "    free(h_new_table);\n",
        "    cudaFree(d_score_table);\n",
        "    cudaFree(d_new_table);\n",
        "\n",
        "    // now will play with strings\n",
        "    int len_data = 3;\n",
        "    int len_seq = 10;\n",
        "    char data0[10] = {'A', 'A', 'A', 'A', 'A', '\\0', '0', '0', '0', '0'};\n",
        "    char data1[10] = {'B', 'A', 'A', 'A', 'A', 'B', '\\0', '0', '0', '0'};\n",
        "    char data2[10] = {'C', 'A', 'A', 'A', '\\0', '0', '0', '0', '0', '0'};\n",
        "\n",
        "    char raw_data[len_data][len_seq];\n",
        "    memcpy(raw_data[0], data0, len_seq);\n",
        "    memcpy(raw_data[1], data1, len_seq);\n",
        "    memcpy(raw_data[2], data2, len_seq);\n",
        "\n",
        "    sequence *h_data = (sequence *)malloc(len_data*sizeof(sequence));\n",
        "    sequence *h_new_data = (sequence *)malloc(len_data*sizeof(sequence));\n",
        "\n",
        "    // copy sequences to data array\n",
        "    for (int i = 0; i < len_data; i++) {\n",
        "        for (int j = 0; j < len_seq; j++) {\n",
        "            h_data[i].sequence[j] = raw_data[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // now allocate cuda arrays\n",
        "    nBytes = len_data*sizeof(sequence);\n",
        "    printf(\"nBytes = %lu\\n\", nBytes);\n",
        "    sequence *d_data;\n",
        "    sequence *d_new_data;\n",
        "    cudaMalloc((sequence **)&d_data, nBytes);\n",
        "    cudaMalloc((sequence **)&d_new_data, nBytes);\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "    // copy to device\n",
        "    cudaMemcpy(h_data, d_data, nBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // run substr kernel\n",
        "    substr_seq<<<1, 3>>>(d_data, d_new_data, 0, 5, 10);\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "    // copy results to host\n",
        "    cudaMemcpy(h_new_data, d_new_data, nBytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print results\n",
        "    for (int i = 0; i < len_data; i++) {\n",
        "        printf(\"%s, %s\\n\", h_data[i].sequence, h_new_data[i].sequence);\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_new_data);\n",
        "    free(h_data);\n",
        "    free(h_new_data);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv3EwhrmkNpo",
        "outputId": "709f0a13-6307-466c-b22b-0290ca4e3911"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nElem = 16384\n",
            "Difference = -26841564.000000\n",
            "0.200000\t0.000000\n",
            "i = 1\n",
            "nBytes = 3072\n",
            ", \n",
            ", \n",
            ", \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUFV_kajlo9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}